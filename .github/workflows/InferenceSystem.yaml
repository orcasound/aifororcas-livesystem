name: InferenceSystem

on:
  pull_request:
    branches:
    - main
    paths:
    - InferenceSystem/**
    - .github/workflows/InferenceSystem.yaml
  push:
    branches:
    - main
    paths:
    - InferenceSystem/**
    - .github/workflows/InferenceSystem.yaml
  workflow_dispatch: # Allow manual workflow invocation from the Github Actions UI

concurrency:
  # Cancel any CI/CD workflow currently in progress for the same PR.
  # Allow running concurrently with any other commits.
  group: build-${{ github.event.pull_request.number || github.sha }}
  cancel-in-progress: true

permissions:  # added using https://github.com/step-security/secure-repo
  contents: read

jobs:
  test-windows:
    runs-on: windows-latest
    steps:
    - name: Check out
      uses: actions/checkout@v5

    - name: Download model
      id: model-path
      run: |
        mkdir -p InferenceSystem/model
        curl -o InferenceSystem/model.zip https://trainedproductionmodels.blob.core.windows.net/dnnmodel/11-15-20.FastAI.R1-12.zip
        unzip InferenceSystem/model.zip -d InferenceSystem
        echo "path=InferenceSystem/model" >> $GITHUB_OUTPUT

    - name: Cache pip
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.8'

    - name: Install FFmpeg
      run: choco install ffmpeg -y

    - name: Create environment
      working-directory: InferenceSystem
      run: |
        python -m venv inference-venv

    - name: Set up environment
      working-directory: InferenceSystem
      shell: cmd
      run: |
        call .\inference-venv\Scripts\activate.bat && python -m pip install --upgrade pip && pip install -r requirements.txt

    - name: Test live inference locally
      working-directory: InferenceSystem
      shell: cmd
      run: |
        call .\inference-venv\Scripts\activate.bat && python src/LiveInferenceOrchestrator.py --config ./config/Test/FastAI_LiveHLS_OrcasoundLab.yml --max_iterations 1

  test-ubuntu:
    runs-on: ubuntu-latest
    steps:
    - name: Check out
      uses: actions/checkout@v5

    - name: Download model
      id: model-path
      run: |
        mkdir -p InferenceSystem/model
        curl -o InferenceSystem/model.zip https://trainedproductionmodels.blob.core.windows.net/dnnmodel/11-15-20.FastAI.R1-12.zip
        unzip InferenceSystem/model.zip -d InferenceSystem
        echo "path=InferenceSystem/model" >> $GITHUB_OUTPUT

    - name: Cache pip
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.8'

    - name: Install FFmpeg
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg

    - name: Create environment
      working-directory: InferenceSystem
      run: |
        python -m venv inference-venv

    - name: Set up environment
      working-directory: InferenceSystem
      run: |
        source ./inference-venv/bin/activate && python -m pip install --upgrade pip && pip install -r requirements.txt

    - name: Test live inference locally
      working-directory: InferenceSystem
      run: |
        source ./inference-venv/bin/activate && python src/LiveInferenceOrchestrator.py --config ./config/Test/FastAI_LiveHLS_OrcasoundLab.yml --max_iterations 1

  test-docker:
    runs-on: ubuntu-latest

    permissions:
      contents: read

    steps:
    - uses: step-security/harden-runner@f4a75cfd619ee5ce8d5b864b0d183aff3c69b55a # v2.13.1
      with:
        egress-policy: audit

    - name: Checkout
      uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0

    - name: Download model
      working-directory: InferenceSystem
      run: |
        curl -o model.zip https://trainedproductionmodels.blob.core.windows.net/dnnmodel/11-15-20.FastAI.R1-12.zip

    - name: Create environment variable file
      working-directory: InferenceSystem
      run: |
        touch .env

    - name: Build docker container
      working-directory: InferenceSystem
      run: |
        docker build . -t live-inference-system -f ./Dockerfile
