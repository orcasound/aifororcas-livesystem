{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fastai Migration Notes (v1)\n",
    "\n",
    "This notebook contains notes and experiments using fastai v1. Unfortunately, \n",
    "I cannot combine notes for both v1 and v2 in the same notebook because they\n",
    "need to be run using different Python environments.\n",
    "\n",
    "For each step that touches fastai or fastai_audio, I will create equivalent\n",
    "code in the corresponding `fastai-migration-notes-v2.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T00:08:57.847565Z",
     "start_time": "2020-07-31T00:08:38.517910Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from audio import (\n",
    "    AudioList, \n",
    "    AudioConfig, \n",
    "    ClassificationInterpretation,\n",
    "    SpectrogramConfig,\n",
    "    get_spectro_transforms,\n",
    "    audio_learner,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Check\n",
    "\n",
    "I'm following the rough structure of `2_FastAI_v2_Script.py`. I'll keep some text from that notebook as quotes for reference.\n",
    "\n",
    "> This step just checks data and provide some summary statistics like sampling rate of different audio clips and length distribution of each waveFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Defining path of modeling related data (Contains two folder positive and negative)\n",
    "data_folder = Path(\"data/sample/\")\n",
    "audios = AudioList.from_folder(data_folder)\n",
    "\n",
    "len(audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dict = audios.stats(prec=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load Data\n",
    "\n",
    "As far as I can tell, everything that's being done by AudioConfig and AudioList can be replicated using torchaudio transforms. I will preview the spectrograms without any masking and make sure they're identical to the output from torchaudio's equivalent code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T16:45:41.151825Z",
     "start_time": "2020-07-30T16:45:40.979298Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Definining Audio config needed to create on the fly mel spectograms\n",
    "config = AudioConfig(standardize=False, \n",
    "                     sg_cfg=SpectrogramConfig(\n",
    "                         f_min=0.0,  ## Minimum frequency to Display\n",
    "                         f_max=10000, ## Maximum Frequency to Display\n",
    "                         hop_length=256,\n",
    "                         n_fft=2560, ## Number of Samples for Fourier\n",
    "                         n_mels=256, ## Mel bins\n",
    "                         pad=0, \n",
    "                         to_db_scale=True, ## Converting to DB sclae\n",
    "                         top_db=100,  ## Top decible sound\n",
    "                         win_length=None, \n",
    "                         n_mfcc=20)\n",
    "                    )\n",
    "config.duration = 4000 ## 4 sec padding or snip\n",
    "config.resample_to=20000 ## Every sample at 20000 frequency\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{config.max_to_pad=}\")\n",
    "print(f\"{config.segment_size=}\")\n",
    "print(f\"{config.sg_cfg.to_db_scale=}\")\n",
    "print(f\"{config.mfcc=}\")\n",
    "print(f\"{config.standardize=}\")\n",
    "print(f\"{config.delta=}\")\n",
    "print(f\"{config.duration=}\")\n",
    "print(f\"{config._processed=}\")\n",
    "print(f\"{config._sr=}\")\n",
    "print(f\"{config.duration=}\")\n",
    "print(f\"{config.sg_cfg.hop_length=}\")\n",
    "print(f\"{config.pad_mode=}\")\n",
    "\n",
    "config.sg_cfg.top_db\n",
    "config.sg_cfg.mel_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "audios = AudioList.from_folder(data_folder, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import ItemList\n",
    "\n",
    "ItemList.get(audios, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "\n",
    "tensor = audios.get(0).get_spec_images()[0].px\n",
    "print(tensor.shape)\n",
    "image = F.to_pil_image(tensor)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios.get(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This code creates a AudioDataBunch which apply defined transformations (In our case frequency masking) on the fly and provide input spectograms to the model in defined bactch size (64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_folder = Path(\"./data/train/mldata/all/\") \n",
    "audios = AudioList.from_folder(data_folder, config=config).split_by_rand_pct(.2, seed=4).label_from_folder()\n",
    "\n",
    "## Defining Transformation\n",
    "tfms = None\n",
    "\n",
    "## Frequency masking:ON\n",
    "tfms = get_spectro_transforms(mask_time=False, mask_freq=True, roll=False) \n",
    "\n",
    "## Creating a databunch\n",
    "db = audios.transform(tfms).databunch(bs=64)\n",
    "\n",
    "## Let's insepect some data\n",
    "db.show_batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Code below creates a ResNet18 model, removes the last 2 fully connected layer and then add new fully connected layers and load the pretrained weights from ImageNet Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Default learner is ResNet 18 \n",
    "learn = audio_learner(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is key feature of FastAI library, this helps us find the ideal learning rate by running model on sample data to see how the accuracy progresses. Output of this step is a learning rate curve (Choose the learning rate where loss starts bumping again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Find ideal learning rate\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T00:23:09.362133Z",
     "start_time": "2020-07-31T00:23:09.135833Z"
    },
    "hidden": true
   },
   "source": [
    "Training model, two cool things to highlight - \n",
    "- **This model is getting trained using [1 cycle learning policy]**(https://arxiv.org/abs/1803.09820) which leads to faster conversion, Here is a [cool blog](https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6) explaing the same if you are not a paper person\n",
    "- **Differential learning rate** - You want different learning rate for different layer of models. In transfer learning you don't want to change learning rate of early layers as fast as later layers in network. (The slice function allows us to pass that information in FastAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## 1-cycle learning (5 epochs and variable learning rate)\n",
    "learn.fit_one_cycle(5, slice(2e-3, 2e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "FastAI outputs the model training porgress per epoch, Note that the accuracy is only calculated on Validation set (20% holdout set created during creating AudioDatabunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Find ideal learning rate\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## 1-cycle learning (5 epochs and variable learning rate)\n",
    "learn.fit_one_cycle(5, slice(1e-5, 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Exporting the model\n",
    "learn.export('models/stg2-rn18.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T00:29:54.507136Z",
     "start_time": "2020-07-31T00:29:54.302755Z"
    },
    "hidden": true
   },
   "source": [
    "With just 15 minutes of training we got our accuracy up to ~93.7% on 20% holdout set which was not used for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A cool function in fastAI to plot different evaluation measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plot top losses help you plot 10 most wrong prediction by the model, this helps you listen/visualize the sound. This helps you understand where the model is not performing the best and provide key insights. As we can listen in below examples some of these audios don't contain Orca Call but the labeling process has marked them positive and some cases where model thinks there is a Orca call but nobody tagged it as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interp.plot_top_losses(10, heatmap = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model Evaluation on testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Defining DataFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_data_folder = Path(\"./data/test/all/\")\n",
    "test_data_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Creating a AudioBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test = AudioList.from_folder(test_data_folder, config=config).split_none().label_from_folder()\n",
    "testdb = test.transform(tfms).databunch(bs=64)\n",
    "\n",
    "## Also extracting true labels\n",
    "true_value = pd.Series(list(testdb.train_ds.y.items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generating predictions : \n",
    "- **To-Do** - There should be a better way to batch scoring, write now we have to score 1 by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for item in tqdm_notebook(testdb.x):\n",
    "    predictions.append(learn.predict(item)[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Calulating performance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"AUC Score :{0:.2f} \\nF-1 Score :{1:.2f} \\nAccuracy Score :{2:.2f} \\nAveragePrecisionScore :{1:.2f}\".format(\n",
    "    roc_auc_score(true_value,pd.Series(predictions)), \n",
    "    f1_score(true_value,pd.Series(predictions)>0.5), \n",
    "    accuracy_score(true_value,pd.Series(predictions)>0.5),\n",
    "    average_precision_score(true_value,pd.Series(predictions) )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T00:36:17.351814Z",
     "start_time": "2020-07-31T00:36:17.123828Z"
    },
    "hidden": true
   },
   "source": [
    "Wohoo model seems to performing inline with our initial model training process on this test set. Let's plot a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(true_value, pd.Series(predictions)>0.5, classes=[\"No Orca\",\"Orca\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring for official evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(\"./data/train/mldata/all/models/\", 'stg2-rn18.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the 2 sec audio clips generated in Data prepration step for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_folder = Path(\"./data/test/OrcasoundLab07052019_Test/test2Sec/\")\n",
    "tfms=None\n",
    "test = AudioList.from_folder(test_data_folder, config=config).split_none().label_empty()\n",
    "testdb = test.transform(tfms).databunch(bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnning though model and generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "pathList = [] \n",
    "for item in tqdm_notebook(testdb.x):\n",
    "    predictions.append(learn.predict(item)[2][1])\n",
    "    pathList.append(str(item.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporing the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({'FilePath': pathList, 'pred': predictions})\n",
    "prediction['FileName'] = prediction.FilePath.apply(lambda x: x.split('/')[6].split(\"-\")[0])\n",
    "prediction.loc[:,['FileName','pred']].to_csv('./test2Sec.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the predictions in standard evaluation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T17:26:20.639759Z",
     "start_time": "2020-07-30T17:26:20.413024Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load predictions\n",
    "test2secDF = pd.read_csv(\"./test2Sec.csv\") \n",
    "\n",
    "## Clean the predictions(it got converted in string)\n",
    "test2secDF['pred'] = test2secDF.pred.apply(lambda x: float(x.split('(')[1].split(')')[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T17:28:29.203097Z",
     "start_time": "2020-07-30T17:28:28.995955Z"
    }
   },
   "outputs": [],
   "source": [
    "## Extracting Start time from file name\n",
    "test2secDF['startTime'] = test2secDF.FileName.apply(lambda x: int(x.split('__')[1].split('.')[0].split('_')[0]))\n",
    "\n",
    "## Sorting the file based on startTime\n",
    "test2secDF = test2secDF.sort_values(['startTime']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T17:47:41.863843Z",
     "start_time": "2020-07-30T17:47:41.661774Z"
    }
   },
   "outputs": [],
   "source": [
    "test2secDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T17:54:51.232979Z",
     "start_time": "2020-07-30T17:54:51.040092Z"
    }
   },
   "outputs": [],
   "source": [
    "## Rolling Window (to average at per second level)\n",
    "submission = pd.DataFrame({'pred': list(test2secDF.rolling(2)['pred'].mean().values)}).reset_index().rename(columns={'index':'StartTime'})\n",
    "\n",
    "## Updating first row\n",
    "submission.loc[0,'pred'] = test2secDF.pred[0]\n",
    "\n",
    "## Adding lastrow\n",
    "lastLine = pd.DataFrame({'StartTime':[submission.StartTime.max()+1],'pred':[test2secDF.pred[test2secDF.shape[0]-1]]})\n",
    "submission = submission.append(lastLine, ignore_index=True)\n",
    "\n",
    "finalSubmission = submission.loc[submission.pred > 0.5,:].reset_index(drop=True)\n",
    "finalSubmission['Duration'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T17:55:37.716590Z",
     "start_time": "2020-07-30T17:55:37.436363Z"
    }
   },
   "outputs": [],
   "source": [
    "## Final submission file\n",
    "finalSubmission.loc[:,['StartTime','Duration']].to_csv('../evaluation/submission/submission2SecFastAI.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orca-v1-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
